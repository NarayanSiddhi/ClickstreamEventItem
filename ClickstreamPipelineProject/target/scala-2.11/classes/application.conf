app {
  name = "ClickstreamDataPipeline"
  version = "1.0"
}

input {
<<<<<<< HEAD
  path1 = "C:\\Users\\Siddhi\\Desktop\\TargetProject\\ClickstreamPipelineProject\\src\\test\\scala\\data_in\\clickstream_log (1).csv"
  path2 = "C:\\Users\\Siddhi\\Desktop\\TargetProject\\ClickstreamPipelineProject\\src\\test\\scala\\data_in\\item_data (1).csv"
}

output {
  path = "C:\\Users\\Siddhi\\Desktop\\TargetProject\\ClickstreamPipelineProject\\src\\test\\scala\\data_out\\click_stream_event_item"
}
=======
  path1 = "C:/ClickstreamPipelineProject/target/scala-2.11/test-classes/data_in/clickstream_log (1).csv"
  path2= "C:/ClickstreamPipelineProject/target/scala-2.11/test-classes/data_in/item_data (1).csv"
  }

output {
  path = "C:/ClickstreamPipelineProject/target/scala-2.11/test-classes/data_out/clickstream_event_dataset"
  }
>>>>>>> origin/feat_ayushi

spark {
  master = "local[*]"
  appName = ${app.name}
  logLevel = "ERROR"
  //  spark.executor.memory = "2g"
  //  spark.default.parallelism = 4
  //  spark.sql.shuffle.partitions = 10
  //  spark.streaming.backpressure.enabled = true
  //  spark.streaming.kafka.maxRatePerPartition=1000
}